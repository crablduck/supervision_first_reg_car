#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§†é¢‘ç›®æ ‡è¿½è¸ªç³»ç»Ÿ
åŸºäºsupervisionåº“å¯¹è§†é¢‘ä¸­çš„è¡Œäººã€å°è½¿è½¦å’Œæ‘©æ‰˜è½¦è¿›è¡Œè½¨è¿¹è¿½è¸ª
"""

import cv2
import numpy as np
import supervision as sv
from ultralytics import YOLO
import argparse
import os
import sys
from pathlib import Path
from tqdm import tqdm
import time

class VideoTracker:
    def __init__(self, model_path="yolov8n.pt"):
        """
        åˆå§‹åŒ–è§†é¢‘è¿½è¸ªå™¨
        
        Args:
            model_path: YOLOæ¨¡å‹è·¯å¾„
        """
        print("ğŸš€ åˆå§‹åŒ–è§†é¢‘è¿½è¸ªç³»ç»Ÿ...")
        
        # åŠ è½½YOLOæ¨¡å‹
        try:
            self.model = YOLO(model_path)
            print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹: {model_path}")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            sys.exit(1)
        
        # åˆå§‹åŒ–è¿½è¸ªå™¨
        self.tracker = sv.ByteTrack()
        
        # ç›®æ ‡ç±»åˆ«æ˜ å°„ (COCOæ•°æ®é›†)
        self.target_classes = {
            0: "person",      # è¡Œäºº
            2: "car",        # å°è½¿è½¦
            3: "motorcycle", # æ‘©æ‰˜è½¦
            5: "bus",        # å…¬äº¤è½¦
            7: "truck"       # å¡è½¦
        }
        
        # é¢œè‰²æ˜ å°„
        self.colors = {
            "person": (0, 255, 0),      # ç»¿è‰²
            "car": (255, 0, 0),        # è“è‰²
            "motorcycle": (0, 0, 255), # çº¢è‰²
            "bus": (255, 255, 0),      # é’è‰²
            "truck": (255, 0, 255)     # ç´«è‰²
        }
        
        print("âœ… è¿½è¸ªå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def process_video(self, video_path, output_path=None):
        """
        å¤„ç†å•ä¸ªè§†é¢‘æ–‡ä»¶
        
        Args:
            video_path: è¾“å…¥è§†é¢‘è·¯å¾„
            output_path: è¾“å‡ºè§†é¢‘è·¯å¾„
        """
        if not os.path.exists(video_path):
            print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
            return False
        
        print(f"\nğŸ“¹ å¼€å§‹å¤„ç†è§†é¢‘: {video_path}")
        
        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            print(f"âŒ æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            return False
        
        # è·å–è§†é¢‘ä¿¡æ¯
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps}FPS, æ€»å¸§æ•°: {total_frames}")
        
        # è®¾ç½®è¾“å‡ºè§†é¢‘
        if output_path is None:
            output_path = video_path.replace('.mp4', '_tracked.mp4')
        
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        detection_stats = {class_name: 0 for class_name in self.target_classes.values()}
        track_history = {}
        
        # è¿›åº¦æ¡
        pbar = tqdm(total=total_frames, desc="å¤„ç†è¿›åº¦", unit="å¸§")
        
        frame_count = 0
        start_time = time.time()
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            
            # YOLOæ£€æµ‹
            results = self.model(frame, verbose=False)
            
            # è½¬æ¢ä¸ºsupervisionæ ¼å¼
            detections = sv.Detections.from_ultralytics(results[0])
            
            # è¿‡æ»¤ç›®æ ‡ç±»åˆ«
            mask = np.isin(detections.class_id, list(self.target_classes.keys()))
            detections = detections[mask]
            
            # æ›´æ–°è¿½è¸ªå™¨
            detections = self.tracker.update_with_detections(detections)
            
            # ç»˜åˆ¶æ£€æµ‹ç»“æœå’Œè½¨è¿¹
            annotated_frame = self.draw_annotations(frame, detections, track_history)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            for class_id in detections.class_id:
                if class_id in self.target_classes:
                    detection_stats[self.target_classes[class_id]] += 1
            
            # å†™å…¥è¾“å‡ºè§†é¢‘
            out.write(annotated_frame)
            
            # æ›´æ–°è¿›åº¦æ¡
            elapsed_time = time.time() - start_time
            fps_current = frame_count / elapsed_time if elapsed_time > 0 else 0
            
            pbar.set_postfix({
                'å½“å‰FPS': f'{fps_current:.1f}',
                'æ£€æµ‹æ•°': len(detections),
                'è¡Œäºº': detection_stats.get('person', 0),
                'è½¦è¾†': detection_stats.get('car', 0) + detection_stats.get('bus', 0) + detection_stats.get('truck', 0),
                'æ‘©æ‰˜è½¦': detection_stats.get('motorcycle', 0)
            })
            pbar.update(1)
        
        # æ¸…ç†èµ„æº
        cap.release()
        out.release()
        pbar.close()
        
        print(f"\nâœ… è§†é¢‘å¤„ç†å®Œæˆ: {output_path}")
        print(f"ğŸ“ˆ æ£€æµ‹ç»Ÿè®¡:")
        for class_name, count in detection_stats.items():
            if count > 0:
                print(f"  - {class_name}: {count} æ¬¡æ£€æµ‹")
        
        return True
    
    def draw_annotations(self, frame, detections, track_history):
        """
        åœ¨å¸§ä¸Šç»˜åˆ¶æ£€æµ‹æ¡†å’Œè½¨è¿¹
        
        Args:
            frame: è¾“å…¥å¸§
            detections: æ£€æµ‹ç»“æœ
            track_history: è½¨è¿¹å†å²
        
        Returns:
            annotated_frame: æ ‡æ³¨åçš„å¸§
        """
        annotated_frame = frame.copy()
        
        if len(detections) == 0:
            return annotated_frame
        
        # ç»˜åˆ¶æ£€æµ‹æ¡†
        for i, (bbox, class_id, confidence, track_id) in enumerate(
            zip(detections.xyxy, detections.class_id, detections.confidence, detections.tracker_id)
        ):
            if class_id not in self.target_classes:
                continue
            
            class_name = self.target_classes[class_id]
            color = self.colors.get(class_name, (255, 255, 255))
            
            # ç»˜åˆ¶è¾¹ç•Œæ¡†
            x1, y1, x2, y2 = map(int, bbox)
            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)
            
            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{class_name} #{track_id} ({confidence:.2f})"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
            cv2.rectangle(annotated_frame, (x1, y1 - label_size[1] - 10), 
                         (x1 + label_size[0], y1), color, -1)
            cv2.putText(annotated_frame, label, (x1, y1 - 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
            
            # æ›´æ–°è½¨è¿¹å†å²
            center = ((x1 + x2) // 2, (y1 + y2) // 2)
            if track_id not in track_history:
                track_history[track_id] = []
            track_history[track_id].append(center)
            
            # é™åˆ¶è½¨è¿¹é•¿åº¦
            if len(track_history[track_id]) > 30:
                track_history[track_id] = track_history[track_id][-30:]
            
            # ç»˜åˆ¶è½¨è¿¹
            if len(track_history[track_id]) > 1:
                points = np.array(track_history[track_id], dtype=np.int32)
                cv2.polylines(annotated_frame, [points], False, color, 2)
        
        return annotated_frame

def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description="è§†é¢‘ç›®æ ‡è¿½è¸ªç³»ç»Ÿ")
    parser.add_argument("--videos", nargs="+", default=["video1.mp4", "video2.mp4"],
                       help="è¾“å…¥è§†é¢‘æ–‡ä»¶åˆ—è¡¨")
    parser.add_argument("--model", default="yolov8n.pt", help="YOLOæ¨¡å‹è·¯å¾„")
    parser.add_argument("--output-dir", default="output", help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(exist_ok=True)
    
    print("ğŸ¯ è§†é¢‘ç›®æ ‡è¿½è¸ªç³»ç»Ÿå¯åŠ¨")
    print("=" * 50)
    
    # åˆå§‹åŒ–è¿½è¸ªå™¨
    tracker = VideoTracker(args.model)
    
    # å¤„ç†æ¯ä¸ªè§†é¢‘
    for video_path in args.videos:
        if os.path.exists(video_path):
            output_path = output_dir / f"{Path(video_path).stem}_tracked.mp4"
            success = tracker.process_video(video_path, str(output_path))
            if not success:
                print(f"âŒ å¤„ç†å¤±è´¥: {video_path}")
        else:
            print(f"âš ï¸  è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
    
    print("\nğŸ‰ æ‰€æœ‰è§†é¢‘å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir.absolute()}")

if __name__ == "__main__":
    main()